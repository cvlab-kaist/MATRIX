"""
interaction_eval.py

Usage:
  python interaction_eval.py \
      --metas_dir metas \
      --videos_dir generated_videos \
      --output results/interaction_analysis.json \
      --model gpt \
      --type CogvideoX
"""

import os
import json
import base64
import argparse
from pathlib import Path
from typing import List, Dict, Any
import re
import cv2
import numpy as np
import tempfile
import shutil

from openai import OpenAI  # pip install openai
from tqdm import tqdm  # pip install tqdm

SYSTEM_INSTRUCTION = """
# Role

You are an expert in verifying whether an interaction truly occurs between entities in a video. The video may be either a real video or generated by an i2v video diffusion model, which may produce hallucinations.

---

# Rules

Definitions:

- Hallucination = an object that was not present in the first frame and is therefore not annotated.
- Wrong target = an object that was present in the first frame but not bbox/mask-annotated.
1. Primary Goal
    
    Determine whether an interaction occurs only between the entities that are bbox/mask-annotated in the first frame. All evaluations are performed per key interaction.
    
2. Key Interaction Extraction
    
    Given a natural language prompt, extract all key interactions involving the provided entities. A key interaction refers to a distinct physical action or relation mentioned in the prompt.
    
    You must extract all meaningful interaction phrases before analyzing the frames. Each interaction is evaluated independently across the video.
    
3. Exclusion Principle (ignore non-annotated objects)
    
    Any interaction involving objects without a bbox must be ignored. Such objects include:
    
    - (a) Hallucinated objects (not present in the first frame),
    - (b) Wrong targets (present in the first frame but not annotated).
    - (c) No interaction(no interaction found throughout the video)
    - (See Rules 4-6.)
4. Hallucination
    
    These involve objects that were not present in the first frame.
    
    - 4-1. Merge with annotated objects: If a new unannotated object overlaps or merges with an annotated entity, treat it as hallucination and reject the interaction.
    - 4-2. Unrealistic deformation: If a bbox-annotated entity becomes unrealistically distorted, split, merged, or changes shape in an impossible way, or if the entity is not coherent and consistently trackable across frames, treat it as hallucination and reject the interaction.
    - 4-3. Harmless hallucinations: If hallucinated objects appear but do not affect the interaction among annotated entities, ignore them. The interaction still counts as valid if it occurs between the annotated entities.
5. Wrong Target Rule
    
    These involve objects that were present in the first frame but not annotated.
    
    If an entity interacts with such an object (even if it belongs to the same class), classify it as wrong_target â†’ no interaction.
    
6. No Interaction Found
    
    If no interaction at all occurs throughout the video â€” neither between annotated entities nor involving any other object â€” classify it as no_interaction_found â†’ no interaction.
    
7. First Frame Annotations
    
    The first frame includes both bbox and mask annotations. Mask colors will also be provided in the JSON file, but these colors are only identifiers and carry no semantic meaning.
    

---

# Input Format

1. Frames:
    
    A sequence of video frames.
    
    - The first frame includes bbox and mask annotations.
    - Subsequent frames include bbox annotations only.
2. Entity/color JSON file:

```json
{
  "prompt": "The video shows a hand tearing a piece of paper in half. The person then holds the two halves together. The background is dark with a spot of light.",
  "entities": ["person", "paper"],
  "colors": ["green", "red"]
}

```

- The `entities` and `colors` arrays are aligned by index. For example, `entities[0]` corresponds to `colors[0]`.
- Colors are only identifiers for masks and carry no semantic meaning.

---

# Output Format

Produce a single JSON output that includes the extracted key interactions and a separate evaluation block for each interaction:

```json
{
  "key_interactions": [
    "hand tearing a piece of paper",
    "holds the two halves together"
  ],
  "evaluation": [
    {
      "interaction": "hand tearing a piece of paper",
      "interaction_type": "interaction / no interaction",
      "interaction_score": 0.92,
      "error_type": "none / hallucination / wrong_target / no_interaction_found",
      "error_confidence": 0.00,
      "reason": "short explanation of why interaction is accepted or rejected",
      "pairwise_descriptions": {
        "pair_0_1": "description for frames 0â†’1",
        "pair_1_2": "description for frames 1â†’2",
        ...
        "pair_{F-2}_{F-1}": "description for frames {F-2}â†’{F-1}"
      } 
    },
    {
      "interaction": "holds the two halves together",
      "interaction_type": "interaction / no interaction",
      "interaction_score": 0.08,
      "error_type": "none / hallucination / wrong_target / no_interaction_found",
      "error_confidence": 0.87,
      "reason": "short explanation of why interaction is accepted or rejected",
      "pairwise_descriptions": {
        "pair_0_1": "description for frames 0â†’1",
        "pair_1_2": "description for frames 1â†’2",
        ...
        "pair_{F-2}_{F-1}": "description for frames {F-2}â†’{F-1}"
      }
    }
  ]
}

```

- interaction_type: whether the key interaction truly occurred (`interaction` / `no interaction`)
- interaction_score:
    A float âˆˆ `[0.0, 1.0]` representing how well the interaction occurred.
    - `0.0` â†’ clearly no interaction
    - `1.0` â†’ clear and successful interaction
    - Intermediate values for partial or ambiguous interactions (e.g., 0.4, 0.7)
- error_type:
    - `none`: interaction occurred as expected
    - `hallucination`: hallucinated object was involved
    - `wrong_target`: unannotated object (present in first frame) was used
    - `no_interaction_found`: no such interaction happened at all
- error_confidence: float in `[0.0, 1.0]` = confidence for `error_type`.
    - If `interaction_type` is "interaction", then `error_type` must be "none" and `error_confidence` must be omitted (or set to `0.0`).
    - If `interaction_type` is "no interaction", then `error_type` must be one of `hallucination`, `wrong_target`, or `no_interaction_found`, and `error_confidence` must be provided.
- reason: concise justification for the interaction decision
- pairwise_descriptions:
    - Let `F` be the number of input frames
    - Output exactly `F-1` entries with keys: `pair_0_1`, `pair_1_2`, â€¦, `pair_{F-2}_{F-1}`
    - Describe the interaction-relevant changes between each consecutive frame pair (e.g., approaching, touching, releasing, lifting)
    - Do not skip subtle changes unless the two frames are truly identical
    - All descriptions must remain consistent with the overall video context

---

Reasoning policy

Use spatial reasoning, temporal continuity, and physical contact logic to evaluate each key interaction. Apply common-sense physical knowledge to validate realistic, causal interactions. Use internal step-by-step reasoning to ensure correctness, but output only the final JSON
"""


def natural_key(path_str: str) -> int:
    stem = Path(path_str).stem
    nums = re.findall(r'\d+', stem)
    return int(nums[-1]) if nums else 0


def load_json_metadata(json_path: Path) -> Dict[str, Any]:
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    if 'detailed_caption' in data:
        del data['detailed_caption']
    
    return data


def load_frames_from_video(video_path: Path, video_id: str, stride: int) -> List[str]:
    if not video_path.exists():
        raise FileNotFoundError(f"Video file not found: {video_path}")
    
    cap = cv2.VideoCapture(str(video_path))
    
    if not cap.isOpened():
        raise ValueError(f"Cannot open video file: {video_path}")
    
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    
    print(f"Video info - Total frames: {total_frames}, FPS: {fps:.2f}")
    
    frame_indices = list(range(0, total_frames, stride))
    
    temp_dir = Path(tempfile.mkdtemp(prefix=f"frames_{video_id}_"))
    frame_files = []
    
    try:
        for i, frame_idx in enumerate(frame_indices):
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            
            ret, frame = cap.read()
            if not ret:
                print(f"Warning: Cannot read frame {frame_idx}")
                continue
            
            temp_frame_path = temp_dir / f"{frame_idx:05d}.png"
            success = cv2.imwrite(str(temp_frame_path), frame)
            
            if success:
                frame_files.append(str(temp_frame_path))
            else:
                print(f"Warning: Failed to save frame {frame_idx}")
    
    finally:
        cap.release()
    
    if not frame_files:
        raise RuntimeError(f"No frames could be extracted from video: {video_path}")
    
    print(f"Extracted {len(frame_files)} frames from video {video_id}")
    return frame_files


def encode_image_to_data_url(image_path: str) -> str:
    with open(image_path, "rb") as f:
        b64 = base64.b64encode(f.read()).decode("utf-8")
    
    suffix = Path(image_path).suffix.lower()
    if suffix in [".jpg", ".jpeg"]:
        mime = "image/jpeg"
    elif suffix == ".png":
        mime = "image/png"
    elif suffix == ".webp":
        mime = "image/webp"
    else:
        mime = "image/png"  
    
    return f"data:{mime};base64,{b64}"


def analyze_frames_with_gpt(
    frame_paths: List[str],
    metadata: Dict[str, Any],
    client: OpenAI,
    model_name: str,
    seed: int = 42
) -> Dict[str, Any]:
    image_urls = []
    for frame_path in tqdm(frame_paths, desc="Converting frames", unit="frame"):
        data_url = encode_image_to_data_url(frame_path)
        image_urls.append({"type": "image_url", "image_url": {"url": data_url}})
    
    gpt_input = {
        "prompt": metadata.get("prompt", []),
        "entity": metadata.get("entity", []),
        "colors": metadata.get("colors", []),
    }
    
    metadata_text = f"Input Data:\n{json.dumps(gpt_input, indent=2, ensure_ascii=False)}"
    
    user_content = [{"type": "text", "text": metadata_text}] + image_urls
    
    messages = [
        {"role": "system", "content": SYSTEM_INSTRUCTION},
        {
            "role": "user", 
            "content": user_content
        }
    ]
    
    print(f"Sending {len(frame_paths)} frames and metadata to GPT...")
    
    response = client.chat.completions.create(
        model=model_name,
        messages=messages,
        seed=42
    )
    
    raw_response = response.choices[0].message.content.strip()
    
    try:
        if "```json" in raw_response:
            json_start = raw_response.find("```json") + 7
            json_end = raw_response.find("```", json_start)
            json_str = raw_response[json_start:json_end].strip()
        elif raw_response.startswith("{") and raw_response.endswith("}"):
            json_str = raw_response
        else:
            return {"raw_response": raw_response, "parsing_error": "No valid JSON found"}
        
        parsed_result = json.loads(json_str)
        return parsed_result
        
    except json.JSONDecodeError as e:
        print(f"Warning: Failed to parse GPT response as JSON: {e}")
        return {"raw_response": raw_response, "parsing_error": str(e)}


def process_single_video(
    video_id: str,
    metas_dir: Path,
    videos_dir: Path,
    client: OpenAI,
    model_name: str,
    stride: int
) -> Dict[str, Any]:
    json_path = metas_dir / f"{video_id}.json"
    if not json_path.exists():
        raise FileNotFoundError(f"Metadata file not found: {json_path}")
    
    metadata = load_json_metadata(json_path)
    print(f"Loaded metadata for video {video_id}")
    
    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv']
    video_path = None
    
    for ext in video_extensions:
        candidate_path = videos_dir / f"{video_id}{ext}"
        if candidate_path.exists():
            video_path = candidate_path
            break
        candidate_path = videos_dir / f"{video_id}{ext.upper()}"
        if candidate_path.exists():
            video_path = candidate_path
            break
    
    if video_path is None:
        raise FileNotFoundError(f"Video file not found for video_id: {video_id}")
    
    frame_paths = load_frames_from_video(video_path, video_id, stride)
    print(f"Selected {len(frame_paths)} frames for video {video_id}")
    
    analysis_result = analyze_frames_with_gpt(
        frame_paths=frame_paths,
        metadata=metadata,
        client=client,
        model_name=model_name
    )
    
    if frame_paths:
        temp_dir = Path(frame_paths[0]).parent
        try:
            shutil.rmtree(temp_dir)
            print(f"Cleaned up temporary files in: {temp_dir}")
        except Exception as e:
            print(f"Warning: Failed to clean up temporary files: {e}")
    
    result = {
        "video_id": metadata.get("video_id"),
        "prompt": metadata.get("prompt"),
        "entity": metadata.get("entity", []),
        "colors": metadata.get("colors", []),
        "frames_analyzed": len(frame_paths),
        "selected_frame_paths": frame_paths,
        "model_used": model_name,
        "gpt_response": analysis_result  
    }
    
    return result


def main():
    parser = argparse.ArgumentParser(description="Analyze video frames using GPT with metadata from JSON files")
    parser.add_argument("--metas_dir", type=str, required=True, help="Path to the metas directory containing JSON files")
    parser.add_argument("--videos_dir", type=str, required=True, help="Path to the videos directory")
    parser.add_argument("--output", type=str, required=True, help="Path to save the analysis results (JSON)")
    parser.add_argument("--model", type=str, default="gpt", help="OpenAI model to use")
    parser.add_argument("--stride", type=int, default=5, help="Stride for frame sampling")
    parser.add_argument("--type", type=str, default="CogvideoX", help="Type of video generation model")
    
    args = parser.parse_args()

    metas_dir = Path(args.metas_dir)
    videos_dir = Path(args.videos_dir)
    
    if not metas_dir.exists():
        raise FileNotFoundError(f"Metas directory not found: {metas_dir}")
    
    if not videos_dir.exists():
        raise FileNotFoundError(f"Videos directory not found: {videos_dir}")
    
    client = OpenAI()  
    json_files = sorted([f for f in metas_dir.iterdir() if f.suffix == '.json'], key=natural_key)
    print(f"Found {len(json_files)} video metadata files to process")
    
    if not json_files:
        raise RuntimeError(f"No valid JSON files found in {metas_dir}")
    
    results = []
    
    for json_file in tqdm(json_files, desc="Processing videos", unit="video"):
        video_id = json_file.stem
        try:
            result = process_single_video(
                video_id=video_id,
                metas_dir=metas_dir,
                videos_dir=videos_dir,
                client=client,
                model_name=args.model,
                stride=args.stride
            )
            results.append(result)
            print(f"âœ“ Successfully processed video {video_id}")                
        except Exception as e:
            print(f"âœ— Error processing video {video_id}: {e}")
            continue
    
    if not results:
        raise RuntimeError("No videos were successfully processed")
    
    video_model = args.type 
    output_path = Path(f"{args.output}/{video_model}.json")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nðŸŽ‰ Analysis complete! Results saved to: {output_path}")
    print(f"ðŸ“Š Successfully processed {len(results)} out of {len(json_files)} videos")
    print(f"ðŸ“‹ Each video entry contains structured interaction analysis with presence/order judgments")


if __name__ == "__main__":
    main()
