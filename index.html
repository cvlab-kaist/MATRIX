<html>

<head>
    <meta charset="utf-8" />
    <title>MATRIX: Mask Track Alignment for Interaction-Aware Video Generation</title>

    <!-- Favicon references -->
    <link rel="icon" type="image/png" href="./images/logo.png">
    <link rel="apple-touch-icon" href="./images/logo.png">
    <link rel="icon" type="image/x-icon" href="favicon.ico">

    <meta
        content="MATRIX: Mask Track Alignment for Interaction-Aware Video Generation"
        name="description" />
    <meta
        content="MATRIX: Mask Track Alignment for Interaction-Aware Video Generation"
        property="og:title" />
    <meta
        content="MATRIX: Mask Track Alignment for Interaction-Aware Video Generation"
        property="og:description" />
    <meta
        content="MATRIX: Mask Track Alignment for Interaction-Aware Video Generation"
        property="twitter:title" />
    <meta
        content="MATRIX: Mask Track Alignment for Interaction-Aware Video Generation"
        property="twitter:description" />
    <meta property="og:type" content="website" />
    <meta content="summary_large_image" name="twitter:card" />
    <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"
        crossorigin="anonymous">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&family=Varela+Round&display=swap"
        rel="stylesheet">
    <link href="style.css?" rel="stylesheet" type="text/css" />

    <!-- üîé Added minimal CSS for click‚Äëto‚Äëzoom lightbox -->
    <style>
      /* make images clearly zoomable */
      img.zoomable { cursor: zoom-in; transition: transform .2s ease; }

      /* fullscreen overlay */
      .lightbox-overlay {
        position: fixed; inset: 0; display: none; align-items: center; justify-content: center;
        background: rgba(0,0,0,.9); z-index: 10000;
      }
      .lightbox-overlay.active { display: flex; }
      .lightbox-overlay img { max-width: 95vw; max-height: 95vh; box-shadow: 0 10px 40px rgba(0,0,0,.6); border-radius: 8px; }
      /* show zoom-out cursor while overlay is open */
      .lightbox-overlay, .lightbox-overlay * { cursor: zoom-out; }

      /* prevent background scroll when overlay is open */
      body.no-scroll { overflow: hidden; }
    </style>

    <!-- MathJax for LaTeX rendering -->
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$','$$'], ['\\[','\\]']]
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            },
            svg: { fontCache: 'global' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" async></script>
</head>

<body>
    <header class="site-header">
        <div class="container">
            <nav class="main-nav">
                <ul class="nav-links">
                    <li><a href="#overview">Overview</a></li>
                    <li><a href="#dataset">Dataset</a></li>
                    <li><a href="#analysis">Analysis</a></li>
                    <li><a href="#framework">Framework</a></li>
                    <li><a href="#results">Results</a></li>
                    <li><a href="#citation">Citation</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <div class="hero-section">
        <div class="container">
            <div class="title-row">
                <div class="title-flex">
                    <video class="title-video" autoplay muted loop playsinline>
                        <source src="images/matrix.png" type="image">
                        <!-- <source src="images/moving viral.mp4" type="video/mp4"> -->
                    </video>
                    <div class="title-text-block">
                        <h1 class="title"><span class="gradient-text">MATRIX</span>: Mask Track Alignment<span class="title-break"></span> for Interaction-Aware Video Generation</h1>
                        <h1 class="subtitle">arXiv 2025</h1>
                    </div>
                </div>
            </div>
            <!-- Author Information -->
            <div class="base-row author-row">
                <div class="base-col author-col">
                    <a href= "https://JinSY515.github.io/" target ="_blank" class="author-text">
                        Siyoon Jin
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="" target="_blank" class="author-text">
                        Seongchan Kim
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="" target="_blank" class="author-text">
                        Jaeho Lee
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="" target="_blank" class="author-text">
                        Dahyun Chung
                    </a>
                </div>
            </div>
            <div class="base-row author-row">
                <div class="base-col author-col">
                    <a href="" target="_blank" class="author-text">
                        Hyunwook Choi
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="" target="_blank" class="author-text">
                        Jiyoung Kim
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="" target="_blank" class="author-text">
                        Jisu Nam
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://cvlab.kaist.ac.kr/" target="_blank" class="author-text">
                        Seungryong Kim
                    </a>
                </div>
            </div>
            <!-- Author Affiliations -->
            <div class="base-row author-row">
                <div class="base-col author-col affiliations">
                    KAIST AI &nbsp;&nbsp; 
                    <br>
                </div>
            </div>
            <!-- Links -->
            <div class="link-labels base-row"> 
                <div class="base-col icon-col"><a href="" target="_blank"
                        class="link-block">
                        <i class="fa fas fa-file-text main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">Paper</strong>
                    </a></div>
                <div class="base-col icon-col"><a href='https://github.com/cvlab-kaist/MATRIX' class="link-block">
                        <i class="fa fa-github main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">Code</strong>
                    </a></div>
                <div class="base-col icon-col"><a href="#citation" class="link-block">
                        <i class="fa fa-graduation-cap main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">Citation</strong>
                    </a></div>
            </div>
        </div>
    </div>

    <main class="main-content">
        <div class="container">
            <!-- Contents: TL;DR -->
            <div class="tldr">
                <b>TL;DR</b>: MATRIX reveals how video diffusion transformers (DiTs) represent multi-instance or subject-object interactions during video generation. Building on this, our MATRIX framework introduces a simple yet effective regularization that aligns the attention of key interaction-dominant layers with multi-instance mask tracks to enhance the interaction-awareness of video DiTs during video generation.
            </div>
            <!-- Contents: Overview -->
            <div id="overview" class="base-row section">
                <h2>Overview</h2>
                <p class="paragraph">
                    <!---
                    Recent video diffusion models can generate high-quality videos but often fail to correctly represent multi-object interactions ‚Äî e.g., losing track of who does what to whom or drifting identities over time. MATRIX tackles this by first analyzing how DiTs internally bind text and video tokens, revealing that semantic grounding and temporal propagation of interactions emerge only in a few critical attention layers. 
                    It then fine-tunes just those layers using SGA + SPA losses aligned with multi-instance mask tracks, enabling efficient improvement without retraining the whole model. To support both analysis and fair benchmarking, the authors release MATRIX-11K, a large dataset of interaction-rich videos, and InterGenEval, a new protocol to measure interaction fidelity beyond standard quality metrics.
                    -->
                    Video DiTs have advanced video generation, yet they still struggle to model multi-instance or subject-object interactions. This raises a key question: <strong>How do these models internally represent interactions?</strong> 
                    To answer this, we curate MATRIX-11K, a video dataset with interaction-aware captions and multi-instance mask tracks. 
                    Using this dataset, we conduct a systematic analysis that formalizes two perspectives of video DiTs: <strong>semantic grounding</strong>, via video-to-text attention, which evaluates whether noun and verb tokens capture instances and their relations; 
                    and <strong>semantic propagation</strong>, via video-to-video attention, which assesses whether instance bindings persist across frames. 
                    We find both effects concentrate in a small subset of interaction-dominant layers. Motivated by this, we introduce <strong>MATRIX</strong>, a simple and effective regularization that aligns attention in specific layers of video DiTs with multi-instance mask tracks from the MATRIX-11K dataset, enhancing both grounding and propagation. 
                    We further propose <strong>InterGenEval</strong>, an evaluation protocol for interaction-aware video generation. 
                    In experiments, MATRIX improves both interaction fidelity and semantic alignment while reducing drift and hallucination. Extensive ablations validate our design choices. 
                </p>
            </div>
            <div class="image-container">
                <div class="image-content">
                    <img src="images/teaser.png" class="img large-image" alt="MATRIX teaser">
                </div>
            </div>
            <div>
                <p>
                    <img class="paragraph-image" src="images/logo.png" alt="MATRIX robot" />
                    <strong>MATRIX-11K</strong> Dataset ‚Äî 11K videos with interaction-aware captions and instance-level mask tracks.
                </p>
                <p>
                    <img class="paragraph-image" src="images/logo.png" alt="MATRIX robot" />
                    <strong>First systematic analysis</strong> of how Video DiTs encode semantic grounding and semantic propagation, analyzed via 3D full attentions, identifying interaction-dominant layers.
                </p>
                <p>
                    <img class="paragraph-image" src="images/logo.png" alt="MATRIX robot" />
                    <strong>MATRIX</strong> Framework ‚Äî a simple yet effective regularization that aligns attention in interaction-dominant layers with multi-instance mask tracks via two terms, including Semantic Grounding Alignment (SGA) and Semantic Propagation Alignment (SPA) losses. 
                </p>
                <p>
                    <img class="paragraph-image" src="images/logo.png" alt="MATRIX robot" />
                    <strong>InterGenEval</strong> Protocol ‚Äî an interaction-aware evaluation protocol that measures Key Interaction Semantic Alignment (KISA), Semantic Grounding Integrity (SGI), along with Semantic Propagation Integrity (SPI), and Interaction Fidelity (IF).
                </p>
                <p>
                    <img class="paragraph-image" src="images/logo.png" alt="MATRIX robot" />
                    <strong>State-of-the-art</strong> performance ‚Äî significant interaction fidelity gains over baselines while maintaining video quality.
                </p>
            </div>
            <!-- Contents: Dataset -->
            <section id="dataset" class="section">
                <h2>Dataset</h2>
                <div class="image-container">
                    <div class="image-content">
                        <img src="images/fig_data_pipeline.png" class="img large-image" alt="Comparisons diagram">
                    </div>
                    <p class="image-caption">(e) Layer-wise alignment between visual tokens in MLLMs and vision encoder features measured by CKNNA, with shaded regions highlighting middle layers that are especially important for visual understanding. (f) Benchmark performance corresponding to (a‚Äìd) </p>
                </div>

                <!-- <h3>Do MLLMs undergo visual information loss?</h3>
                <p>MLLMs are trained with text-only supervision, causing their visual features to drift from the encoder‚Äôs rich representations.
                    We measure this with CKNNA and observe a sharp drop in similarity to CLIP features across layers.</p>
                <h3>Is preserving visual information beneficial?</h3>
                <p>Motivated by the observation that internal visual representations of MLLMs diverge from the encoder‚Äôs original features, we ask whether re-injecting them could help. We add a residual connection that feeds the input visual features of the language model back into its mid-layers. This preserves alignment with the encoder and yields consistent performance gains on multimodal benchmarks.</p>
                <h3>Can we compensate visual information with raw vision encoder feature?</h3>
                <p>We first tested re-injecting raw vision encoder features into mid-layers via a residual branch, but this degraded performance as the features were not language-aligned and disrupted intermediate representations. 
                To address this, we introduce a more principled strategy by explicitly aligning intermediate visual representations with frozen encoder features through a Visual Representation Alignment (VRA) loss. <p>This approach yields general improvements, though limitations remain: <strong> <i>encoder features can also transmit their inherent biases</i></strong>, as observed in MMVP.</p> -->
            </section>
            <!-- Contents: Analysis -->
            <section id="analysis" class="section">
                <h2>Analysis</h2>
                <div class="two-col">
                    <div class="col-left">
                        <p>
                        <h3>What should serve as the alignment target?</h3>
                        </p>
                        <p>
                        While aligning multimodal models with their own encoder features preserves some visual cues, it is fundamentally constrained by the encoder‚Äôs representational capacity. To overcome this limitation, we leverage stronger vision foundation models (VFMs) as teachers, providing richer, vision-centric targets.
                        </p>
                        <p>
                        Building on this insight, we introduce <strong><i>VIRAL</i></strong>, which aligns intermediate MLLM representations to features from pretrained VFMs, thereby preserving more informative visual semantics than those available from the input encoder alone.
                        </p>
                    </div>
                    <div class="col-right img">
                        <div class="image-container">
                            <img src="matrix_images/viral_architecture_hd.png" class="image-item img large-image z-depth-1" alt="VIRAL framework illustration">
                            <p class="image-caption">Building upon our findings in visual representation alignment, we align visual pathway representation from MLLMs to strong, informative representations from VFMs to improve the vision understanding performance of MLLMs. </p>
                        </div>
                    </div>
                </div>
            </section>
            <!-- Contents: Framework -->
            <section id="framework" class="section">
                <h2>Framework</h2>
                <p> VIRAL demonstrates its effectiveness through evaluations on benchmarks covering vision-centric (CV-Bench<sup>2D</sup>, What's Up, and MMVP), hallucination (POPE), and general multimodal tasks (MMStar, MME). Across identical training conditions, 
                    VIRAL consistently surpasses the baseline, with its strongest improvements on fine-grained visual understanding. These gains are achieved by leveraging DINOv2 as the VFM.</p> 
                <h3>Generally applicable to MLLMs across diverse vision encoders.</h3>
                <p>Our method consistently boosts MLLM performance on vision-centric tasks by aligning intermediate visual features with a strong vision encoder. Gains persist even with SigLIPv2, showing that regularizing visual representations benefits MLLMs beyond compensating for contrastive pretraining limits.</p>
                <div class="table-container">
                    <table class="advantages-table">
                        <thead>
                            <tr>
                                <th class="model-col">Language Model</th>
                                <th class="model-col">Vision Encoder</th>
                                <th class="vra-col">VRA Loss</th>
                                <th>CV-Bench<sup>2D</sup></th>
                                <th>MMVP</th>
                                <th>What's Up</th>
                                <th>POPE</th>
                                <th>MMStar</th>
                                <th>MME</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td class="model-col" rowspan="4"><strong>Vicuna-7B-1.5</strong></td>
                                <td class="model-col" rowspan="2">CLIP</td>
                                <td class="vra-col"><i class="fa fa-times vra-no" aria-label="VRA off"></i></td>
                                <td>56.82%</td>
                                <td>28.20%</td>
                                <td>40.13%</td>
                                <td>85.70%</td>
                                <td><strong>33.93%</strong></td>
                                <td>1650.21</td>
                            </tr>
                            <tr>
                                <td class="vra-col"><i class="fa fa-check vra-yes" aria-label="VRA on"></i></td>
                                <td><strong>59.67%</strong></td>
                                <td><strong>33.33%</strong></td>
                                <td><strong>48.55%</strong></td>
                                <td><strong>87.43%</strong></td>
                                <td><strong>33.93%</strong></td>
                                <td><strong>1694.52</strong></td>
                            </tr>
                            <tr>
                                <td class="model-col" rowspan="2">SigLipv2</td>
                                <td class="vra-col"><i class="fa fa-times vra-no" aria-label="VRA off"></i></td>
                                <td>58.90%</td>
                                <td>28.22%</td>
                                <td>40.90%</td>
                                <td>90.13%</td>
                                <td>37.20%</td>
                                <td>1738.96</td>
                            </tr>
                            <tr>
                                <td class="vra-col"><i class="fa fa-check vra-yes" aria-label="VRA on"></i></td>
                                <td><strong>62.66%</strong></td>
                                <td><strong>33.11%</strong></td>
                                <td><strong>44.40%</strong></td>
                                <td><strong>90.77%</strong></td>
                                <td><strong>37.20%</strong></td>
                                <td><strong>1835.62</strong></td>
                            </tr>
                        </tbody>
                    </table>
                    <p class="image-caption">Although using a much stronger vision encoder, we observe that adopting our regularization loss also leads to consistent improvements.</p>
                </div>  
                  
                <h3>Generally applicable to MLLMs across model scales and LLM backbones.</h3>
                <p>Experiments with Qwen2.5-7B and a scaled-up Vicuna-1.5-13B show that VIRAL follows a scaling trend and generalizes across different LLM backbones.</p>
                <div class="table-container">
                    <table class="advantages-table">
                        <thead>
                            <tr>
                                <th class="model-col">Vision Encoder</th>
                                <th class="model-col">Language Model</th>
                                <th class="vra-col">VRA Loss</th>
                                <th>CV-Bench<sup>2D</sup></th>
                                <th>MMVP</th>
                                <th>What's Up</th>
                                <th>POPE</th>
                                <th>MMStar</th>
                                <th>MME</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td class="model-col" rowspan="4">CLIP</td>
                                <td class="model-col" rowspan="2"><strong>Qwen2.5-7B</strong></td>
                                <td class="vra-col"><i class="fa fa-times vra-no" aria-label="VRA off"></i></td>
                                <td>58.97%</td>
                                <td>33.47%</td>
                                <td>59.08%</td>
                                <td><strong>85.88%</strong></td>
                                <td>39.20%</td>
                                <td>1743.56</td>
                            </tr>
                            <tr>
                                <td class="vra-col"><i class="fa fa-check vra-yes" aria-label="VRA on"></i></td>
                                <td><strong>60.50%</strong></td>
                                <td><strong>36.07%</strong></td>
                                <td><strong>63.57%</strong></td>
                                <td>84.92%</td>
                                <td><strong>39.67%</strong></td>
                                <td><strong>1765.65</strong></td>
                            </tr>
                            <tr>
                                <td class="model-col" rowspan="2"><strong>Vicuna-1.5-13B</strong></td>
                                <td class="vra-col"><i class="fa fa-times vra-no" aria-label="VRA off"></i></td>
                                <td>57.51%</td>
                                <td>39.33%</td>
                                <td>44.44%</td>
                                <td>87.12%</td>
                                <td>34.47%</td>
                                <td>1599.04</td>
                            </tr>
                            <tr>
                                <td class="vra-col"><i class="fa fa-check vra-yes" aria-label="VRA on"></i></td>
                                <td><strong>58.97%</strong></td>
                                <td><strong>45.33%</strong></td>
                                <td><strong>62.26%</strong></td>
                                <td><strong>87.79%</strong></td>
                                <td><strong>37.00%</strong></td>
                                <td><strong>1636.62</strong></td>
                            </tr>
                        </tbody>
                    </table>
                    <p class="image-caption">Across different LLM backbones, the regularization loss remains effective and yields consistent improvements.</p>
                </div>
                
                <h3>Key Improvements</h3>
                <p>With VIRAL, performance on vision-centric tasks like counting and spatial reasoning improves significantly, outperforming LLaVA-7B-1.5 on challenging visual questions.</p>
                <div class="image-container">
                    <img src="matrix_images/viral_qual_hd.png" class="img large-image" alt="Qualitative examples">
                    <p class="image-caption">The left part presents PCA visualizations of intermediate representations, demonstrating that VIRAL yields more structured, semantically meaningful visual embeddings. The right part illustrates instance counting and spatial relation tasks, highlighting scenarios where VIRAL correctly answers questions while the baseline fails.</p>
                </div>
            </section>
            <!-- Contents: Results -->
            <section id="results" class="section">
                <h2>Results</h2>

                <p>We identify optimal visual features and layers for aligning MLLM representations. We adopt DINOv2 as the default encoder. Layer-wise analysis shows the 16th layer yields the best results, with single-layer alignment outperforming multi-layer targets. Finally, we compare direct feature alignment with relation-based objectives over self-similarity matrices.</p>
                <div class="table-container">
                    <table class="advantages-table">
                        <thead>
                        <tr>
                            <th class="model-col">VFM</th>
                            <th class="model-col">Layer</th>
                            <th class="model-col">Objective</th>
                            <th>CV-Bench<sup>2D</sup></th>
                            <th>MMVP</th>
                            <th>What's Up</th>
                            <th>POPE</th>
                            <th>MME</th>
                        </tr>
                        </thead>
                        <tbody>
                        <!-- Baseline -->
                        <tr>
                            <td class="model-col">Baseline</strong></td>
                            <td></td>
                            <td></td>
                            <td>56.82%</td>
                            <td>28.20%</td>
                            <td>40.13%</td>
                            <td>85.70%</td>
                            <td>1650.21</td>
                        </tr>

                        <!-- Different VFMs -->
                        <tr><td colspan="8"><em><span class="bold-text">Ablation on different VFMs</span></em></td></tr>
                        <tr>
                            <td><span class="bold-text">DINOv2</span></td><td>16</td><td>Cos. Sim.</td>
                            <td><strong>59.67%</strong></td><td><strong>33.33%</strong></td>
                            <td><u>48.55%</u></td><td>88.32%</td>
                            <td><strong>1694.52</strong></td>
                        </tr>
                        <tr>
                            <td>CLIP</td><td>16</td><td>Cos. Sim.</td>
                            <td>57.51%</td><td>29.33%</td><td>44.50%</td><td>87.17%</td><td>1548.49</td>
                        </tr>
                        <tr>
                            <td>SAM</td><td>16</td><td>Cos. Sim.</td>
                            <td>57.58%</td><td>30.27%</td><td><strong>49.84%</strong></td><td>88.34%</td><td>1648.77</td>
                        </tr>
                        <tr>
                            <td>DAv2</td><td>16</td><td>Cos. Sim.</td>
                            <td><u>58.55%</u></td><td>28.67%</td><td>47.29%</td><td><strong>88.70%</strong></td><td>1682.42</td>
                        </tr>
                        <tr>
                            <td>RADIO</td><td>16</td><td>Cos. Sim.</td>
                            <td>57.59%</td><td><u>31.80%</u></td><td>47.35%</td><td><u>88.52%</u></td><td><u>1692.94</u></td>
                        </tr>

                        <!-- Single-layer targets -->
                        <table class="advantages-table">
                        <thead>
                        <tr>
                            <th class="model-col">VFM</th>
                            <th class="model-col">Layer</th>
                            <th class="model-col">Objective</th>
                            <th>CV-Bench<sup>2D</sup></th>
                            <th>MMVP</th>
                            <th>What's Up</th>
                            <th>POPE</th>
                            <th>MME</th>
                        </tr>
                        </thead>
                        <tbody>
                        <tr><td colspan="8"><em>Ablation on single-layer targets</em></td></tr>
                        <tr><td>DINOv2</td><td>4</td><td>Cos. Sim.</td><td><u>58.55%</u></td><td><u>30.67%</u></td><td>45.05%</td><td>87.68%</td><td>1720.36</td></tr>
                        <tr><td>DINOv2</td><td>8</td><td>Cos. Sim.</td><td>58.28%</td><td>27.70%</td><td><u>48.32%</u></td><td><u>88.43%</u></td><td>1662.67</td></tr>
                        <tr><td>DINOv2</td><td>12</td><td>Cos. Sim.</td><td>57.77%</td><td>28.59%</td><td>48.19%</td><td>88.27%</td><td>1648.88</td></tr>
                        <tr><td>DINOv2</td><td>16</td><td>Cos. Sim.</td><td><strong>59.67%</strong></td><td><strong>33.33%</strong></td><td><strong>48.55%</strong></td><td>88.32%</td><td>1694.52</td></tr>
                        <tr><td>DINOv2</td><td>20</td><td>Cos. Sim.</td><td>55.22%</td><td>27.41%</td><td>48.04%</td><td>88.39%</td><td>1705.97</td></tr>
                        <tr><td>DINOv2</td><td>24</td><td>Cos. Sim.</td><td>55.77%</td><td>27.48%</td><td>47.99%</td><td>88.10%</td><td><u>1740.55</u></td></tr>
                        <tr><td>DINOv2</td><td>28</td><td>Cos. Sim.</td><td>54.87%</td><td>27.19%</td><td>47.82%</td><td><strong>88.56%</strong></td><td><strong>1755.86</strong></td></tr>
                        <tr><td>DINOv2</td><td>32</td><td>Cos. Sim.</td><td>56.12%</td><td>26.52%</td><td>47.60%</td><td>87.32%</td><td>1678.69</td></tr>

                        <!-- Multi-layer targets -->
                        <tr><td colspan="8"><em>Ablation on multi-layer targets</em></td></tr>
                        <tr><td>DINOv2</td><td>15‚Äì17</td><td>Cos. Sim.</td><td>59.32%</td><td>28.00%</td><td>47.17%</td><td>87.61%</td><td>1639.72</td></tr>
                        <tr><td>DINOv2</td><td>14‚Äì18</td><td>Cos. Sim.</td><td>49.62%</td><td>22.55%</td><td>42.58%</td><td>87.90%</td><td>1444.32</td></tr>
                        </tbody>    
                        <table class="advantages-table">
                        <thead>
                        <tr>
                            <th class="model-col">VFM</th>
                            <th class="model-col">Layer</th>
                            <th class="model-col">Objective</th>
                            <th>CV-Bench<sup>2D</sup></th>
                            <th>MMVP</th>
                            <th>What's Up</th>
                            <th>POPE</th>
                            <th>MME</th>
                        </tr>
                        </thead>
                        <tbody>
                        <!-- Alignment objectives -->
                        <tr><td colspan="8"><em>Ablation on different objectives</em></td></tr>
                        <tr><td>DINOv2</td><td>16</td><td>Relation</td><td>58.83%</td><td>26.60%</td><td><strong>49.05%</strong></td><td>87.58%</td><td>1674.30</td></tr>
                        <tr><td>DINOv2</td><td>16</td><td>Cos. Sim.</td><td><strong>59.67%</strong></td><td><strong>33.33%</strong></td><td>48.55%</td><td><strong>88.32%</strong></td><td><strong>1694.52</strong></td></tr>
                        </tbody>
                    </table>
                    <p class="image-caption">Ablation results across VFMs, layers, and objectives. Consistent gains are observed with VIRAL regularization.</p>
                </div>
            </section>
            <!-- Contents: Conclusion -->
            <section id="conclusion" class="section">
                <h2>Conclusion</h2>

                <p>In this work, we propose VIRAL, a simple yet effective training strategy that aligns the internal visual representations of MLLMs with those from powerful vision foundation models. Our approach preserves fine-grained visual semantics often discarded under text-only supervision, enabling more accurate spatial reasoning and object grounding. Extensive experiments across diverse benchmarks validate the effectiveness and generality of our method, demonstrating that visual representation alignment significantly enhances both performance and training efficiency in multimodal learning.</p>
            </section>
            <!-- Contents: Citation -->
            <div class="citation add-top-padding">
                <h1 id="citation">Citation</h1>
                <p> If you use this work or find it helpful, please consider citing: </p>
                <pre id="codecell0">
    @misc{yoon2025visualrepresentationalignmentmultimodal,
      title={Visual Representation Alignment for Multimodal Large Language Models}, 
      author={Heeji Yoon and Jaewoo Jung and Junwan Kim and Hyungyu Choi and Heeseong Shin and Sangbeom Lim and Honggyu An and Chaehyun Kim and Jisang Han and Donghyun Kim and Chanho Eom and Sunghwan Hong and Seungryong Kim},
      year={2025},
      eprint={2509.07979},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2509.07979}}
                </pre>
            </div>
        </div>
    </main>

    <footer class="site-footer">
        <div class="container">
            <p class="credit">Credit: The design of this project page is inspired by previous academic project pages, such as <a href="https://llm-grounded-diffusion.github.io/" target="_blank">LLM-grounded Diffusion</a> and <a href="https://describe-anything.github.io/" target="_blank">Describe-anything</a>.</p>
        </div>
    </footer>

    <script>
    function toggleMute(element) {
        const video = element.parentElement.querySelector('video');
        const icon = element.querySelector('i');
        const text = element.querySelector('.unmute-text');
        
        if (video.muted) {
            video.muted = false;
            icon.className = 'fa fa-volume-up';
            text.textContent = 'Mute';
        } else {
            video.muted = true;
            icon.className = 'fa fa-volume-off';
            text.textContent = 'Click to unmute';
        }
    }
    
    document.addEventListener('DOMContentLoaded', function() {
        const videos = document.querySelectorAll('video');
        videos.forEach(video => {
            video.addEventListener('play', function() {
                const overlay = this.parentElement.querySelector('.unmute-overlay');
                if (overlay) overlay.style.opacity = '0.8';
            });
            
            video.addEventListener('pause', function() {
                const overlay = this.parentElement.querySelector('.unmute-overlay');
                if (overlay) overlay.style.opacity = '0.8';
            });
        });

        // Initialize all slideshows
        document.querySelectorAll('.slideshow-container').forEach(container => {
            const slideshow = container.querySelector('.slideshow');
            const slides = slideshow.querySelectorAll('.slide');
            const prevButton = container.querySelector('.slideshow-nav.prev');
            const nextButton = container.querySelector('.slideshow-nav.next');
            const playPauseButton = container.querySelector('.play-pause');
            
            let currentSlide = 0;
            let autoplayInterval;
            let isPlaying = true;

            function showSlide(n) {
                slides.forEach(slide => slide.classList.remove('active'));
                currentSlide = (n + slides.length) % slides.length;
                slides[currentSlide].classList.add('active');
            }

            function changeSlide(n) {
                showSlide(currentSlide + n);
                resetAutoplay();
            }

            function togglePlayPause() {
                if (isPlaying) {
                    clearInterval(autoplayInterval);
                    playPauseButton.innerHTML = '<i class="fa fa-play"></i>';
                } else {
                    startAutoplay();
                    playPauseButton.innerHTML = '<i class="fa fa-pause"></i>';
                }
                isPlaying = !isPlaying;
            }

            function startAutoplay() {
                autoplayInterval = setInterval(() => {
                    showSlide(currentSlide + 1);
                }, 5000);
            }

            function resetAutoplay() {
                clearInterval(autoplayInterval);
                if (isPlaying) {
                    startAutoplay();
                }
            }

            // Initialize this slideshow
            showSlide(0);
            startAutoplay();

            // Add event listeners
            prevButton.addEventListener('click', () => changeSlide(-1));
            nextButton.addEventListener('click', () => changeSlide(1));
            playPauseButton.addEventListener('click', togglePlayPause);
        });

        // Handle main video play button
        const mainVideo = document.querySelector('.main-video');
        const playButton = document.querySelector('.play-button-overlay');
        
        if (mainVideo && playButton) {
            // Click play button to play video
            playButton.addEventListener('click', () => {
                mainVideo.play();
                mainVideo.classList.add('playing');
            });

            // Handle video play/pause events
            mainVideo.addEventListener('play', () => {
                mainVideo.classList.add('playing');
            });

            mainVideo.addEventListener('pause', () => {
                mainVideo.classList.remove('playing');
            });

            mainVideo.addEventListener('ended', () => {
                mainVideo.classList.remove('playing');
            });
        }

        // -----------------------------
        // üñºÔ∏è Click-to-zoom Lightbox
        // -----------------------------
        // Build overlay once
        const lightbox = document.createElement('div');
        lightbox.className = 'lightbox-overlay';
        lightbox.setAttribute('role', 'dialog');
        lightbox.setAttribute('aria-modal', 'true');
        lightbox.innerHTML = '<img alt="Expanded image">';
        document.body.appendChild(lightbox);
        const lightboxImg = lightbox.querySelector('img');

        function openLightbox(src, alt) {
            lightboxImg.src = src;
            lightboxImg.alt = alt || '';
            lightbox.classList.add('active');
            document.body.classList.add('no-scroll');
        }
        function closeLightbox() {
            lightbox.classList.remove('active');
            document.body.classList.remove('no-scroll');
            lightboxImg.src = '';
        }

        // Close on click anywhere or on Esc
        lightbox.addEventListener('click', closeLightbox);
        document.addEventListener('keydown', (e) => {
            if (e.key === 'Escape' && lightbox.classList.contains('active')) closeLightbox();
        });

        // Mark target images as zoomable and wire up click
        const zoomableImages = document.querySelectorAll('.image-container img, .slideshow img, .main-content img.img, .hero-section img.img');
        zoomableImages.forEach(img => {
            img.classList.add('zoomable');
            img.addEventListener('click', () => {
                // support optional high-res source via data-fullsrc
                const src = img.getAttribute('data-fullsrc') || img.currentSrc || img.src;
                openLightbox(src, img.alt);
            });
        });
    });
    </script>
</body>
</html>
